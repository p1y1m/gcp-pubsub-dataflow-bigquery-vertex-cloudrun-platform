{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import standard library\n",
        "import os\n",
        "\n",
        "# Set base directory name\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Create base folder\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Create src folder\n",
        "os.makedirs(os.path.join(BASE_DIR, \"src\"), exist_ok=True)\n"
      ],
      "metadata": {
        "id": "eTN6F52f_vWy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import helper modules\n",
        "import textwrap\n",
        "\n",
        "# Define helper to write file\n",
        "def write_file(path, content):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(textwrap.dedent(content).lstrip())\n",
        "\n",
        "# Write PubSubClientMock class into src/pubsub_mock.py\n",
        "write_file(\n",
        "    os.path.join(BASE_DIR, \"src\", \"pubsub_mock.py\"),\n",
        "    \"\"\"\n",
        "class PubSubClientMock:\n",
        "    def __init__(self):\n",
        "        # Initialize internal list of messages\n",
        "        self.messages = []\n",
        "\n",
        "    def publish(self, topic: str, data: dict):\n",
        "        # Append message to internal list\n",
        "        self.messages.append({\"topic\": topic, \"data\": data})\n",
        "        # Print message for visibility\n",
        "        print(f\"[PubSubMock] Publish to {topic}: {data}\")\n",
        "    \"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "s5GuCheHAUPK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sys to modify path\n",
        "import sys\n",
        "\n",
        "# Add base directory to Python path\n",
        "sys.path.append(BASE_DIR)\n",
        "\n",
        "# Check that mock file exists\n",
        "print(os.path.exists(os.path.join(BASE_DIR, \"src\", \"pubsub_mock.py\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPS8-5a_AjEm",
        "outputId": "ccdab251-cbe0-4f8e-9c06-4b3a4f2b9760"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PubSubClientMock from src\n",
        "from src.pubsub_mock import PubSubClientMock\n",
        "\n",
        "# Create client instance\n",
        "client = PubSubClientMock()\n",
        "\n",
        "# Publish one sim event\n",
        "client.publish(\"events-topic\", {\"user_id\": 1, \"event\": \"view_product\"})\n",
        "\n",
        "# Print internal messages list\n",
        "print(client.messages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wotM-SvsBXwP",
        "outputId": "e402b577-22a8-45fc-e9f1-70118df5e1c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PubSubMock] Publish to events-topic: {'user_id': 1, 'event': 'view_product'}\n",
            "[{'topic': 'events-topic', 'data': {'user_id': 1, 'event': 'view_product'}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import helper to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Define helper to write file (if not defined)\n",
        "def write_file(path, content):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(textwrap.dedent(content).lstrip())\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Write DataflowJobSim class file\n",
        "write_file(\n",
        "    os.path.join(BASE_DIR, \"src\", \"dataflow_sim.py\"),\n",
        "    \"\"\"\n",
        "class DataflowJobSim:\n",
        "    def __init__(self, name: str):\n",
        "        # Store job name\n",
        "        self.name = name\n",
        "\n",
        "    def run(self, input_messages):\n",
        "        # Create list for transformed rows\n",
        "        transformed = []\n",
        "        # Loop over input messages\n",
        "        for m in input_messages:\n",
        "            # Copy data from message\n",
        "            row = dict(m[\"data\"])\n",
        "            # Mark row as processed\n",
        "            row[\"processed\"] = True\n",
        "            # Append row to transformed list\n",
        "            transformed.append(row)\n",
        "        # Print number of processed rows\n",
        "        print(f\"[DataflowSim] Process {len(transformed)} elements\")\n",
        "        # Return transformed rows\n",
        "        return transformed\n",
        "    \"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZXR104ztCSTN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sys to update path\n",
        "import sys, os\n",
        "\n",
        "# Add base directory to Python path\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "sys.path.append(BASE_DIR)\n",
        "\n",
        "# Import Pub/Sub and Dataflow sims\n",
        "from src.pubsub_mock import PubSubClientMock\n",
        "from src.dataflow_sim import DataflowJobSim\n",
        "\n",
        "# Create Pub/Sub client\n",
        "client = PubSubClientMock()\n",
        "\n",
        "# Publish two sim events\n",
        "client.publish(\"events-topic\", {\"user_id\": 1, \"event\": \"view_product\"})\n",
        "client.publish(\"events-topic\", {\"user_id\": 2, \"event\": \"purchase\"})\n",
        "\n",
        "# Create Dataflow job sim\n",
        "job = DataflowJobSim(\"etl-events\")\n",
        "\n",
        "# Run job on Pub/Sub messages\n",
        "rows = job.run(client.messages)\n",
        "\n",
        "# Print transformed rows\n",
        "print(rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNZ3Qvm8CjGF",
        "outputId": "a042f9d4-992e-42ef-c303-edd4bbee4048"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PubSubMock] Publish to events-topic: {'user_id': 1, 'event': 'view_product'}\n",
            "[PubSubMock] Publish to events-topic: {'user_id': 2, 'event': 'purchase'}\n",
            "[DataflowSim] Process 2 elements\n",
            "[{'user_id': 1, 'event': 'view_product', 'processed': True}, {'user_id': 2, 'event': 'purchase', 'processed': True}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Define helper to write file\n",
        "def write_file(path, content):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(textwrap.dedent(content).lstrip())\n",
        "\n",
        "# Write BigQuerySim class file\n",
        "write_file(\n",
        "    os.path.join(BASE_DIR, \"src\", \"bigquery_sim.py\"),\n",
        "    \"\"\"\n",
        "class BigQuerySim:\n",
        "    def __init__(self):\n",
        "        # Initialize dict of tables\n",
        "        self.tables = {}\n",
        "\n",
        "    def insert_rows(self, table: str, rows: list):\n",
        "        # Ensure table key exist\n",
        "        self.tables.setdefault(table, [])\n",
        "        # Extend table with new rows\n",
        "        self.tables[table].extend(rows)\n",
        "        # Print number of inserted rows\n",
        "        print(f\"[BigQuerySim] Insert {len(rows)} rows into {table}\")\n",
        "    \"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Lv0uGyGADGBC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sys to update path\n",
        "import sys, os\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Add base directory to Python path\n",
        "sys.path.append(BASE_DIR)\n",
        "\n",
        "# Import sims\n",
        "from src.pubsub_mock import PubSubClientMock\n",
        "from src.dataflow_sim import DataflowJobSim\n",
        "from src.bigquery_sim import BigQuerySim\n",
        "\n",
        "# Create Pub/Sub client\n",
        "client = PubSubClientMock()\n",
        "\n",
        "# Publish two sim events\n",
        "client.publish(\"events-topic\", {\"user_id\": 1, \"event\": \"view_product\"})\n",
        "client.publish(\"events-topic\", {\"user_id\": 2, \"event\": \"purchase\"})\n",
        "\n",
        "# Create Dataflow job sim\n",
        "job = DataflowJobSim(\"etl-events\")\n",
        "\n",
        "# Run job on Pub/Sub messages\n",
        "rows = job.run(client.messages)\n",
        "\n",
        "# Create BigQuery sim client\n",
        "bq = BigQuerySim()\n",
        "\n",
        "# Insert rows into one table\n",
        "bq.insert_rows(\"analytics.events\", rows)\n",
        "\n",
        "# Print internal tables dict\n",
        "print(bq.tables)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFAmIEmUDQep",
        "outputId": "cb167392-18ba-48f4-bf5f-22d75686dd90"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PubSubMock] Publish to events-topic: {'user_id': 1, 'event': 'view_product'}\n",
            "[PubSubMock] Publish to events-topic: {'user_id': 2, 'event': 'purchase'}\n",
            "[DataflowSim] Process 2 elements\n",
            "[BigQuerySim] Insert 2 rows into analytics.events\n",
            "{'analytics.events': [{'user_id': 1, 'event': 'view_product', 'processed': True}, {'user_id': 2, 'event': 'purchase', 'processed': True}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Create VertexAISim file with basic ML training\n",
        "with open(os.path.join(BASE_DIR, \"src\", \"vertex_ai_sim.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class VertexAISim:\n",
        "    def __init__(self):\n",
        "        # Initialize dict of models\n",
        "        self.models = {}\n",
        "\n",
        "    def train_model(self, model_name: str, X, y):\n",
        "        # Create logistic regression model\n",
        "        model = LogisticRegression()\n",
        "        # Fit model on data\n",
        "        model.fit(X, y)\n",
        "        # Store model in dict\n",
        "        self.models[model_name] = model\n",
        "        # Print info\n",
        "        print(f\"[VertexAISim] Train {model_name}\")\n",
        "        # Return model name\n",
        "        return model_name\n",
        "\n",
        "    def predict(self, model_name: str, x_row):\n",
        "        # Get model from dict\n",
        "        model = self.models[model_name]\n",
        "        # Compute probability for class 1\n",
        "        prob = model.predict_proba([x_row])[0][1]\n",
        "        # Print info\n",
        "        print(f\"[VertexAISim] Predict with {model_name} on {x_row}\")\n",
        "        # Return sim prediction\n",
        "        return {\"score\": float(prob)}\n",
        "\"\"\").lstrip())\n"
      ],
      "metadata": {
        "id": "3XZyGrr1Er5e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload VertexAISim module after editing file\n",
        "\n",
        "# Import importlib\n",
        "import importlib\n",
        "\n",
        "# Import module object\n",
        "import src.vertex_ai_sim\n",
        "\n",
        "# Reload module to use new version\n",
        "importlib.reload(src.vertex_ai_sim)\n",
        "\n",
        "# Import VertexAISim class from reloaded module\n",
        "from src.vertex_ai_sim import VertexAISim\n"
      ],
      "metadata": {
        "id": "yh4CE_ByFTDA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import numpy for arrays\n",
        "import numpy as np\n",
        "\n",
        "# Import VertexAISim\n",
        "from src.vertex_ai_sim import VertexAISim\n",
        "\n",
        "# Create small training set (price -> high_value flag)\n",
        "X = np.array([[10.0], [20.0], [100.0], [200.0]])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "# Create Vertex AI sim client\n",
        "vertex = VertexAISim()\n",
        "\n",
        "# Train sim model\n",
        "model_name = vertex.train_model(\"price_high_value_model\", X, y)\n",
        "\n",
        "# Predict for one new price\n",
        "prediction = vertex.predict(model_name, [50.0])\n",
        "\n",
        "# Print model name and prediction\n",
        "print(\"Model:\", model_name)\n",
        "print(\"Prediction:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHTL-HVQE3fJ",
        "outputId": "ab470eda-c389-48f0-f025-4aac4002fae0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VertexAISim] Train price_high_value_model\n",
            "[VertexAISim] Predict with price_high_value_model on [50.0]\n",
            "Model: price_high_value_model\n",
            "Prediction: {'score': 0.15735576245764596}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sims for Pub/Sub, Dataflow and BigQuery\n",
        "from src.pubsub_mock import PubSubClientMock\n",
        "from src.dataflow_sim import DataflowJobSim\n",
        "from src.bigquery_sim import BigQuerySim\n",
        "\n",
        "# Create components\n",
        "client = PubSubClientMock()\n",
        "job = DataflowJobSim(\"etl-events\")\n",
        "bq = BigQuerySim()\n",
        "\n",
        "# Publish events with price\n",
        "client.publish(\"events-topic\", {\"user_id\": 1, \"event\": \"view_product\", \"price\": 10.0})\n",
        "client.publish(\"events-topic\", {\"user_id\": 2, \"event\": \"purchase\", \"price\": 30.0})\n",
        "client.publish(\"events-topic\", {\"user_id\": 3, \"event\": \"purchase\", \"price\": 80.0})\n",
        "client.publish(\"events-topic\", {\"user_id\": 4, \"event\": \"purchase\", \"price\": 150.0})\n",
        "\n",
        "# Run Dataflow sim and load into BigQuery sim\n",
        "rows = job.run(client.messages)\n",
        "bq.insert_rows(\"analytics.events\", rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo9IvbErGMoV",
        "outputId": "6ed2a97a-d64d-43b7-b3a8-8cf8f937b09c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PubSubMock] Publish to events-topic: {'user_id': 1, 'event': 'view_product', 'price': 10.0}\n",
            "[PubSubMock] Publish to events-topic: {'user_id': 2, 'event': 'purchase', 'price': 30.0}\n",
            "[PubSubMock] Publish to events-topic: {'user_id': 3, 'event': 'purchase', 'price': 80.0}\n",
            "[PubSubMock] Publish to events-topic: {'user_id': 4, 'event': 'purchase', 'price': 150.0}\n",
            "[DataflowSim] Process 4 elements\n",
            "[BigQuerySim] Insert 4 rows into analytics.events\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import numpy and reload VertexAISim module\n",
        "import numpy as np\n",
        "import importlib, src.vertex_ai_sim\n",
        "importlib.reload(src.vertex_ai_sim)\n",
        "from src.vertex_ai_sim import VertexAISim\n",
        "\n",
        "# Build X and y from BigQuery sim table\n",
        "table = bq.tables[\"analytics.events\"]\n",
        "X = np.array([[row[\"price\"]] for row in table])\n",
        "y = np.array([1 if row[\"price\"] >= 50.0 else 0 for row in table])\n",
        "\n",
        "# Train model using sim Vertex AI\n",
        "vertex = VertexAISim()\n",
        "model_name = vertex.train_model(\"price_high_value_model\", X, y)\n",
        "\n",
        "# Predict for one new price\n",
        "prediction = vertex.predict(model_name, [40.0])\n",
        "print(\"Model:\", model_name)\n",
        "print(\"Prediction:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctpdLaO5GRJc",
        "outputId": "0eeef412-dd47-4d46-9046-954e39e4c9f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VertexAISim] Train price_high_value_model\n",
            "[VertexAISim] Predict with price_high_value_model on [40.0]\n",
            "Model: price_high_value_model\n",
            "Prediction: {'score': 0.03652943301539391}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Create CloudRunSim file\n",
        "with open(os.path.join(BASE_DIR, \"src\", \"cloud_run_sim.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "class CloudRunSim:\n",
        "    def __init__(self, vertex_client, model_name: str):\n",
        "        # Store Vertex AI sim client\n",
        "        self.vertex_client = vertex_client\n",
        "        # Store model name\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def predict_request(self, payload: dict):\n",
        "        # Print incoming payload\n",
        "        print(f\"[CloudRunSim] Receive request: {payload}\")\n",
        "        # Extract features from payload\n",
        "        x_row = [payload[\"price\"]]\n",
        "        # Call Vertex AI sim client\n",
        "        result = self.vertex_client.predict(self.model_name, x_row)\n",
        "        # Build response dict\n",
        "        response = {\n",
        "            \"input\": payload,\n",
        "            \"prediction\": result,\n",
        "        }\n",
        "        # Print response\n",
        "        print(f\"[CloudRunSim] Send response: {response}\")\n",
        "        # Return response\n",
        "        return response\n",
        "\"\"\").lstrip())\n"
      ],
      "metadata": {
        "id": "eTEgkgIOHOcU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CloudRunSim\n",
        "from src.cloud_run_sim import CloudRunSim\n",
        "\n",
        "# Create Cloud Run sim instance using existing vertex client and model_name\n",
        "cloud_run = CloudRunSim(vertex_client=vertex, model_name=model_name)\n",
        "\n",
        "# Build one example request payload\n",
        "request_payload = {\"user_id\": 999, \"event\": \"purchase\", \"price\": 120.0}\n",
        "\n",
        "# Call predict_request\n",
        "api_response = cloud_run.predict_request(request_payload)\n",
        "\n",
        "# Print final API response\n",
        "print(api_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmcfTl0_HnrX",
        "outputId": "571d4d21-e9bc-448d-e39d-940507fbba05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CloudRunSim] Receive request: {'user_id': 999, 'event': 'purchase', 'price': 120.0}\n",
            "[VertexAISim] Predict with price_high_value_model on [120.0]\n",
            "[CloudRunSim] Send response: {'input': {'user_id': 999, 'event': 'purchase', 'price': 120.0}, 'prediction': {'score': 0.9999992824655318}}\n",
            "{'input': {'user_id': 999, 'event': 'purchase', 'price': 120.0}, 'prediction': {'score': 0.9999992824655318}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Overwrite CloudFunctionSim file with better logic\n",
        "with open(os.path.join(BASE_DIR, \"src\", \"cloud_function_sim.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "class CloudFunctionSim:\n",
        "    def __init__(self, downstream_callable):\n",
        "        # Store downstream callable (for example Cloud Run sim)\n",
        "        self.downstream_callable = downstream_callable\n",
        "\n",
        "    def handle_request(self, request: dict):\n",
        "        # Print incoming request\n",
        "        print(f\"[CloudFunctionSim] Receive request: {request}\")\n",
        "\n",
        "        # Validate that price key exist\n",
        "        if \"price\" not in request:\n",
        "            print(\"[CloudFunctionSim] Missing price\")\n",
        "            return {\"error\": \"missing_price\"}\n",
        "\n",
        "        # Validate that price be positive\n",
        "        if request[\"price\"] <= 0:\n",
        "            print(\"[CloudFunctionSim] Invalid price\")\n",
        "            return {\"error\": \"invalid_price\"}\n",
        "\n",
        "        # Create processed copy of request\n",
        "        processed = dict(request)\n",
        "\n",
        "        # Add high_value flag\n",
        "        processed[\"high_value\"] = processed[\"price\"] >= 50.0\n",
        "\n",
        "        # Add simple price_bucket label\n",
        "        if processed[\"price\"] < 50.0:\n",
        "            processed[\"price_bucket\"] = \"low\"\n",
        "        elif processed[\"price\"] < 100.0:\n",
        "            processed[\"price_bucket\"] = \"medium\"\n",
        "        else:\n",
        "            processed[\"price_bucket\"] = \"high\"\n",
        "\n",
        "        # Print processed request\n",
        "        print(f\"[CloudFunctionSim] Process request: {processed}\")\n",
        "\n",
        "        # Call downstream callable\n",
        "        response = self.downstream_callable(processed)\n",
        "\n",
        "        # Return response\n",
        "        return response\n",
        "\"\"\").lstrip())\n"
      ],
      "metadata": {
        "id": "5BWrBcVzJ9O9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CloudFunctionSim\n",
        "from src.cloud_function_sim import CloudFunctionSim\n",
        "\n",
        "# Create Cloud Function sim that call Cloud Run sim\n",
        "cloud_function = CloudFunctionSim(downstream_callable=cloud_run.predict_request)\n",
        "\n",
        "# Build example request payload\n",
        "cf_request = {\"user_id\": 555, \"event\": \"purchase\", \"price\": 70.0}\n",
        "\n",
        "# Call handle_request\n",
        "cf_response = cloud_function.handle_request(cf_request)\n",
        "\n",
        "# Print final Cloud Function response\n",
        "print(cf_response)\n"
      ],
      "metadata": {
        "id": "5Rrm6-aCJ-Dm",
        "outputId": "cc39adcf-afee-47c5-f077-15098c7107ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CloudFunctionSim] Receive request: {'user_id': 555, 'event': 'purchase', 'price': 70.0}\n",
            "[CloudFunctionSim] Process request: {'user_id': 555, 'event': 'purchase', 'price': 70.0, 'high_value': True, 'price_bucket': 'medium'}\n",
            "[CloudRunSim] Receive request: {'user_id': 555, 'event': 'purchase', 'price': 70.0, 'high_value': True, 'price_bucket': 'medium'}\n",
            "[VertexAISim] Predict with price_high_value_model on [70.0]\n",
            "[CloudRunSim] Send response: {'input': {'user_id': 555, 'event': 'purchase', 'price': 70.0, 'high_value': True, 'price_bucket': 'medium'}, 'prediction': {'score': 0.9630317444057592}}\n",
            "{'input': {'user_id': 555, 'event': 'purchase', 'price': 70.0, 'high_value': True, 'price_bucket': 'medium'}, 'prediction': {'score': 0.9630317444057592}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Create CloudStorageSim file\n",
        "with open(os.path.join(BASE_DIR, \"src\", \"cloud_storage_sim.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "import json\n",
        "\n",
        "class CloudStorageSim:\n",
        "    def __init__(self):\n",
        "        # Initialize dict of buckets\n",
        "        self.buckets = {}\n",
        "\n",
        "    def upload_json(self, bucket: str, path: str, obj: dict):\n",
        "        # Ensure bucket key exist\n",
        "        self.buckets.setdefault(bucket, {})\n",
        "        # Convert dict to json string\n",
        "        data = json.dumps(obj)\n",
        "        # Store data under path\n",
        "        self.buckets[bucket][path] = data\n",
        "        # Print info\n",
        "        print(f\"[CloudStorageSim] Upload {path} to {bucket}\")\n",
        "\n",
        "    def read_json(self, bucket: str, path: str):\n",
        "        # Get json string from storage\n",
        "        data = self.buckets[bucket][path]\n",
        "        # Convert json string back to dict\n",
        "        return json.loads(data)\n",
        "\"\"\").lstrip())\n"
      ],
      "metadata": {
        "id": "hMpoaX6rA-Q_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CloudStorageSim\n",
        "from src.cloud_storage_sim import CloudStorageSim\n",
        "\n",
        "# Create Cloud Storage sim client\n",
        "gcs = CloudStorageSim()\n",
        "\n",
        "# Build simple aggregate from BigQuery sim table\n",
        "table = bq.tables[\"analytics.events\"]\n",
        "total_revenue = sum(row[\"price\"] for row in table)\n",
        "num_events = len(table)\n",
        "\n",
        "# Create summary dict\n",
        "summary_obj = {\"num_events\": num_events, \"total_revenue\": total_revenue}\n",
        "\n",
        "# Upload summary to one bucket and path\n",
        "gcs.upload_json(\"raw-analytics-bucket\", \"daily/summary.json\", summary_obj)\n",
        "\n",
        "# Read it back and print\n",
        "loaded = gcs.read_json(\"raw-analytics-bucket\", \"daily/summary.json\")\n",
        "print(loaded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSZM28VPBCYO",
        "outputId": "0c7e9c4c-4247-4d49-eea0-680e61a5671c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CloudStorageSim] Upload daily/summary.json to raw-analytics-bucket\n",
            "{'num_events': 4, 'total_revenue': 270.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Create DataprocSim file\n",
        "with open(os.path.join(BASE_DIR, \"src\", \"dataproc_sim.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "class DataprocSim:\n",
        "    def __init__(self, job_name: str):\n",
        "        # Store job name\n",
        "        self.job_name = job_name\n",
        "\n",
        "    def run(self, summary_obj: dict):\n",
        "        # Print received summary\n",
        "        print(f\"[DataprocSim] Run {self.job_name} on {summary_obj}\")\n",
        "        # Compute average revenue\n",
        "        avg_revenue = summary_obj[\"total_revenue\"] / max(summary_obj[\"num_events\"], 1)\n",
        "        # Build result dict\n",
        "        result = {\n",
        "            \"num_events\": summary_obj[\"num_events\"],\n",
        "            \"total_revenue\": summary_obj[\"total_revenue\"],\n",
        "            \"avg_revenue\": avg_revenue,\n",
        "        }\n",
        "        # Print result\n",
        "        print(f\"[DataprocSim] Result {result}\")\n",
        "        # Return result\n",
        "        return result\n",
        "\"\"\").lstrip())\n"
      ],
      "metadata": {
        "id": "BaAcAP5ADQgj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import DataprocSim\n",
        "from src.dataproc_sim import DataprocSim\n",
        "\n",
        "# Read summary json from Cloud Storage sim\n",
        "summary_obj = gcs.read_json(\"raw-analytics-bucket\", \"daily/summary.json\")\n",
        "\n",
        "# Create Dataproc sim job\n",
        "dp_job = DataprocSim(\"daily-analytics-job\")\n",
        "\n",
        "# Run job on summary\n",
        "dp_result = dp_job.run(summary_obj)\n",
        "\n",
        "# Insert result into new BigQuery sim table\n",
        "bq.insert_rows(\"analytics.daily_summary_dp\", [dp_result])\n",
        "\n",
        "# Print new table content\n",
        "print(bq.tables[\"analytics.daily_summary_dp\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGdZ0zorDR0w",
        "outputId": "a9a247a0-744c-49b4-8642-e191da285797"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DataprocSim] Run daily-analytics-job on {'num_events': 4, 'total_revenue': 270.0}\n",
            "[DataprocSim] Result {'num_events': 4, 'total_revenue': 270.0, 'avg_revenue': 67.5}\n",
            "[BigQuerySim] Insert 1 rows into analytics.daily_summary_dp\n",
            "[{'num_events': 4, 'total_revenue': 270.0, 'avg_revenue': 67.5}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Create CloudSchedulerSim file\n",
        "with open(os.path.join(BASE_DIR, \"src\", \"cloud_scheduler_sim.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "class CloudSchedulerSim:\n",
        "    def __init__(self, job_name: str, job_callable):\n",
        "        # Store job name\n",
        "        self.job_name = job_name\n",
        "        # Store callable to run\n",
        "        self.job_callable = job_callable\n",
        "\n",
        "    def run_now(self):\n",
        "        # Print trigger message\n",
        "        print(f\"[CloudSchedulerSim] Trigger job {self.job_name}\")\n",
        "        # Call job callable\n",
        "        result = self.job_callable()\n",
        "        # Return result\n",
        "        return result\n",
        "\"\"\").lstrip())\n"
      ],
      "metadata": {
        "id": "vJJ7SUsqEURA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CloudSchedulerSim\n",
        "from src.cloud_scheduler_sim import CloudSchedulerSim\n",
        "\n",
        "# Define scheduled job that call Cloud Function sim and Dataproc sim\n",
        "def daily_pipeline_job():\n",
        "    # Call Cloud Function sim for one request\n",
        "    cf_request = {\"user_id\": 777, \"event\": \"purchase\", \"price\": 90.0}\n",
        "    cf_response = cloud_function.handle_request(cf_request)\n",
        "\n",
        "    # Read summary from Cloud Storage sim\n",
        "    summary_obj = gcs.read_json(\"raw-analytics-bucket\", \"daily/summary.json\")\n",
        "\n",
        "    # Run Dataproc sim on summary\n",
        "    dp_result = dp_job.run(summary_obj)\n",
        "\n",
        "    # Insert Dataproc result into BigQuery sim\n",
        "    bq.insert_rows(\"analytics.daily_summary_dp\", [dp_result])\n",
        "\n",
        "    # Build combined result\n",
        "    return {\"cloud_function\": cf_response, \"dataproc\": dp_result}\n",
        "\n",
        "# Create Cloud Scheduler sim\n",
        "scheduler = CloudSchedulerSim(\"daily-pipeline-job\", daily_pipeline_job)\n",
        "\n",
        "# Trigger job now\n",
        "scheduler_result = scheduler.run_now()\n",
        "\n",
        "# Print final result\n",
        "print(scheduler_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQoLVIw9EXQ5",
        "outputId": "418b0491-46fd-4a58-ddae-ed877a61a146"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CloudSchedulerSim] Trigger job daily-pipeline-job\n",
            "[CloudFunctionSim] Receive request: {'user_id': 777, 'event': 'purchase', 'price': 90.0}\n",
            "[CloudFunctionSim] Process request: {'user_id': 777, 'event': 'purchase', 'price': 90.0, 'high_value': True, 'price_bucket': 'medium'}\n",
            "[CloudRunSim] Receive request: {'user_id': 777, 'event': 'purchase', 'price': 90.0, 'high_value': True, 'price_bucket': 'medium'}\n",
            "[VertexAISim] Predict with price_high_value_model on [90.0]\n",
            "[CloudRunSim] Send response: {'input': {'user_id': 777, 'event': 'purchase', 'price': 90.0, 'high_value': True, 'price_bucket': 'medium'}, 'prediction': {'score': 0.9995072391577136}}\n",
            "[DataprocSim] Run daily-analytics-job on {'num_events': 4, 'total_revenue': 270.0}\n",
            "[DataprocSim] Result {'num_events': 4, 'total_revenue': 270.0, 'avg_revenue': 67.5}\n",
            "[BigQuerySim] Insert 1 rows into analytics.daily_summary_dp\n",
            "{'cloud_function': {'input': {'user_id': 777, 'event': 'purchase', 'price': 90.0, 'high_value': True, 'price_bucket': 'medium'}, 'prediction': {'score': 0.9995072391577136}}, 'dataproc': {'num_events': 4, 'total_revenue': 270.0, 'avg_revenue': 67.5}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to write file\n",
        "import os, textwrap\n",
        "\n",
        "# Set base directory\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "\n",
        "# Create Composer sim file\n",
        "with open(os.path.join(BASE_DIR, \"src\", \"composer_sim.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "class TaskSim:\n",
        "    def __init__(self, name, func):\n",
        "        # Store task name\n",
        "        self.name = name\n",
        "        # Store callable function\n",
        "        self.func = func\n",
        "\n",
        "    def run(self, context):\n",
        "        # Print start message\n",
        "        print(f\"[TaskSim] Run {self.name}\")\n",
        "        # Execute function with context\n",
        "        result = self.func(context)\n",
        "        # Return result\n",
        "        return result\n",
        "\n",
        "class ComposerDAGSim:\n",
        "    def __init__(self, name):\n",
        "        # Store dag name\n",
        "        self.name = name\n",
        "        # Initialize list of tasks\n",
        "        self.tasks = []\n",
        "\n",
        "    def add_task(self, task: TaskSim):\n",
        "        # Append task to list\n",
        "        self.tasks.append(task)\n",
        "\n",
        "    def run(self):\n",
        "        # Initialize shared context dict\n",
        "        context = {}\n",
        "        # Loop over tasks in order\n",
        "        for task in self.tasks:\n",
        "            # Run task and store result\n",
        "            context[task.name] = task.run(context)\n",
        "        # Return full context\n",
        "        return context\n",
        "\"\"\").lstrip())\n"
      ],
      "metadata": {
        "id": "k5jwF_TDGmzs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sys and numpy\n",
        "import sys, os, numpy as np\n",
        "\n",
        "# Set base directory and add to path\n",
        "BASE_DIR = \"gcp_colab_sim\"\n",
        "sys.path.append(BASE_DIR)\n",
        "\n",
        "# Import sims\n",
        "from src.composer_sim import ComposerDAGSim, TaskSim\n",
        "from src.pubsub_mock import PubSubClientMock\n",
        "from src.dataflow_sim import DataflowJobSim\n",
        "from src.bigquery_sim import BigQuerySim\n",
        "from src.vertex_ai_sim import VertexAISim\n",
        "from src.cloud_run_sim import CloudRunSim\n",
        "from src.cloud_function_sim import CloudFunctionSim\n",
        "from src.cloud_storage_sim import CloudStorageSim\n",
        "from src.dataproc_sim import DataprocSim\n",
        "\n",
        "# Define ingest task (Pub/Sub + Dataflow + BigQuery)\n",
        "def task_ingest(context):\n",
        "    # Create components\n",
        "    client = PubSubClientMock()\n",
        "    job = DataflowJobSim(\"etl-events-dag\")\n",
        "    bq_local = BigQuerySim()\n",
        "    # Publish events\n",
        "    client.publish(\"events-topic\", {\"user_id\": 1, \"event\": \"view_product\", \"price\": 10.0})\n",
        "    client.publish(\"events-topic\", {\"user_id\": 2, \"event\": \"purchase\", \"price\": 60.0})\n",
        "    client.publish(\"events-topic\", {\"user_id\": 3, \"event\": \"purchase\", \"price\": 120.0})\n",
        "    # Run Dataflow sim\n",
        "    rows = job.run(client.messages)\n",
        "    # Insert into BigQuery sim\n",
        "    bq_local.insert_rows(\"analytics.events\", rows)\n",
        "    # Return objects\n",
        "    return {\"bq\": bq_local}\n",
        "\n",
        "# Define train task (Vertex AI from BigQuery)\n",
        "def task_train(context):\n",
        "    # Get BigQuery sim from ingest\n",
        "    bq_local = context[\"ingest\"][\"bq\"]\n",
        "    table = bq_local.tables[\"analytics.events\"]\n",
        "    # Build X and y\n",
        "    X = np.array([[row[\"price\"]] for row in table])\n",
        "    y = np.array([1 if row[\"price\"] >= 50.0 else 0 for row in table])\n",
        "    # Train model\n",
        "    vertex_local = VertexAISim()\n",
        "    model_name_local = vertex_local.train_model(\"price_high_value_model_dag\", X, y)\n",
        "    # Return model info\n",
        "    return {\"vertex\": vertex_local, \"model_name\": model_name_local, \"bq\": bq_local}\n",
        "\n",
        "# Define batch analytics task (Cloud Storage + Dataproc)\n",
        "def task_batch(context):\n",
        "    # Get BigQuery sim from train task\n",
        "    bq_local = context[\"train\"][\"bq\"]\n",
        "    table = bq_local.tables[\"analytics.events\"]\n",
        "    # Build summary\n",
        "    total_revenue = sum(row[\"price\"] for row in table)\n",
        "    num_events = len(table)\n",
        "    summary_obj = {\"num_events\": num_events, \"total_revenue\": total_revenue}\n",
        "    # Use Cloud Storage sim\n",
        "    gcs_local = CloudStorageSim()\n",
        "    gcs_local.upload_json(\"raw-analytics-bucket-dag\", \"daily/summary.json\", summary_obj)\n",
        "    # Run Dataproc sim\n",
        "    dp_job_local = DataprocSim(\"daily-analytics-job-dag\")\n",
        "    dp_result_local = dp_job_local.run(summary_obj)\n",
        "    # Insert into BigQuery sim\n",
        "    bq_local.insert_rows(\"analytics.daily_summary_dp_dag\", [dp_result_local])\n",
        "    # Return results\n",
        "    return {\"gcs\": gcs_local, \"dataproc_result\": dp_result_local}\n",
        "\n",
        "# Define predict task (Cloud Function + Cloud Run + Vertex)\n",
        "def task_predict(context):\n",
        "    # Get Vertex AI objects\n",
        "    vertex_local = context[\"train\"][\"vertex\"]\n",
        "    model_name_local = context[\"train\"][\"model_name\"]\n",
        "    # Create Cloud Run sim\n",
        "    cloud_run_local = CloudRunSim(vertex_client=vertex_local, model_name=model_name_local)\n",
        "    # Create Cloud Function sim\n",
        "    cloud_function_local = CloudFunctionSim(downstream_callable=cloud_run_local.predict_request)\n",
        "    # Build request\n",
        "    cf_request = {\"user_id\": 999, \"event\": \"purchase\", \"price\": 90.0}\n",
        "    # Call Cloud Function sim\n",
        "    response = cloud_function_local.handle_request(cf_request)\n",
        "    # Return response\n",
        "    return {\"api_response\": response}\n"
      ],
      "metadata": {
        "id": "3rcmGZ1_GqAh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Composer DAG sim\n",
        "dag = ComposerDAGSim(\"full_gcp_sim_dag\")\n",
        "\n",
        "# Add tasks in order\n",
        "dag.add_task(TaskSim(\"ingest\", task_ingest))\n",
        "dag.add_task(TaskSim(\"train\", task_train))\n",
        "dag.add_task(TaskSim(\"batch\", task_batch))\n",
        "dag.add_task(TaskSim(\"predict\", task_predict))\n",
        "\n",
        "# Run whole DAG\n",
        "dag_context = dag.run()\n",
        "\n",
        "# Print final context keys\n",
        "print(\"DAG context keys:\", dag_context.keys())\n",
        "print(\"Predict task output:\", dag_context[\"predict\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbwnKtnoGsvA",
        "outputId": "41f4c805-94f7-4f44-b48e-aa73edecedbe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TaskSim] Run ingest\n",
            "[PubSubMock] Publish to events-topic: {'user_id': 1, 'event': 'view_product', 'price': 10.0}\n",
            "[PubSubMock] Publish to events-topic: {'user_id': 2, 'event': 'purchase', 'price': 60.0}\n",
            "[PubSubMock] Publish to events-topic: {'user_id': 3, 'event': 'purchase', 'price': 120.0}\n",
            "[DataflowSim] Process 3 elements\n",
            "[BigQuerySim] Insert 3 rows into analytics.events\n",
            "[TaskSim] Run train\n",
            "[VertexAISim] Train price_high_value_model_dag\n",
            "[TaskSim] Run batch\n",
            "[CloudStorageSim] Upload daily/summary.json to raw-analytics-bucket-dag\n",
            "[DataprocSim] Run daily-analytics-job-dag on {'num_events': 3, 'total_revenue': 190.0}\n",
            "[DataprocSim] Result {'num_events': 3, 'total_revenue': 190.0, 'avg_revenue': 63.333333333333336}\n",
            "[BigQuerySim] Insert 1 rows into analytics.daily_summary_dp_dag\n",
            "[TaskSim] Run predict\n",
            "[CloudFunctionSim] Receive request: {'user_id': 999, 'event': 'purchase', 'price': 90.0}\n",
            "[CloudFunctionSim] Process request: {'user_id': 999, 'event': 'purchase', 'price': 90.0, 'high_value': True, 'price_bucket': 'medium'}\n",
            "[CloudRunSim] Receive request: {'user_id': 999, 'event': 'purchase', 'price': 90.0, 'high_value': True, 'price_bucket': 'medium'}\n",
            "[VertexAISim] Predict with price_high_value_model_dag on [90.0]\n",
            "[CloudRunSim] Send response: {'input': {'user_id': 999, 'event': 'purchase', 'price': 90.0, 'high_value': True, 'price_bucket': 'medium'}, 'prediction': {'score': 0.999993569853917}}\n",
            "DAG context keys: dict_keys(['ingest', 'train', 'batch', 'predict'])\n",
            "Predict task output: {'api_response': {'input': {'user_id': 999, 'event': 'purchase', 'price': 90.0, 'high_value': True, 'price_bucket': 'medium'}, 'prediction': {'score': 0.999993569853917}}}\n"
          ]
        }
      ]
    }
  ]
}